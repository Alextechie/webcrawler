# webcrawler
Building a web crawler


# The robots.txt file is used to write instructions that allow certain parts
of the website to be scrolled and others not to be scrolled.